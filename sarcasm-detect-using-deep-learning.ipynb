{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport re\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-18T16:52:56.063425Z","iopub.execute_input":"2022-10-18T16:52:56.063951Z","iopub.status.idle":"2022-10-18T16:53:02.832247Z","shell.execute_reply.started":"2022-10-18T16:52:56.063912Z","shell.execute_reply":"2022-10-18T16:53:02.830826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_1 = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", lines=True)\ndata_2 = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf =  pd.concat([data_1, data_2])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:02.834411Z","iopub.execute_input":"2022-10-18T16:53:02.835056Z","iopub.status.idle":"2022-10-18T16:53:03.310115Z","shell.execute_reply.started":"2022-10-18T16:53:02.835018Z","shell.execute_reply":"2022-10-18T16:53:03.308845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:03.311563Z","iopub.execute_input":"2022-10-18T16:53:03.311925Z","iopub.status.idle":"2022-10-18T16:53:03.319382Z","shell.execute_reply.started":"2022-10-18T16:53:03.311894Z","shell.execute_reply":"2022-10-18T16:53:03.317898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:03.320970Z","iopub.execute_input":"2022-10-18T16:53:03.321379Z","iopub.status.idle":"2022-10-18T16:53:03.343708Z","shell.execute_reply.started":"2022-10-18T16:53:03.321345Z","shell.execute_reply":"2022-10-18T16:53:03.342306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df['article_link']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:03.346576Z","iopub.execute_input":"2022-10-18T16:53:03.347536Z","iopub.status.idle":"2022-10-18T16:53:03.359467Z","shell.execute_reply.started":"2022-10-18T16:53:03.347496Z","shell.execute_reply":"2022-10-18T16:53:03.358180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(df.is_sarcastic)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:03.361004Z","iopub.execute_input":"2022-10-18T16:53:03.361606Z","iopub.status.idle":"2022-10-18T16:53:04.121301Z","shell.execute_reply.started":"2022-10-18T16:53:03.361569Z","shell.execute_reply":"2022-10-18T16:53:04.119875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# contraction_dict = don't -> do not\n\ncontractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n\n\ncontractions_re=re.compile('(%s)' % '|'.join(contractions.keys()))\ndef expand_contractions(text,contractions_dict=contractions):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n# Expanding Contractions in the reviews","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:04.123141Z","iopub.execute_input":"2022-10-18T16:53:04.123639Z","iopub.status.idle":"2022-10-18T16:53:04.148185Z","shell.execute_reply.started":"2022-10-18T16:53:04.123589Z","shell.execute_reply":"2022-10-18T16:53:04.146700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n#html\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:04.150935Z","iopub.execute_input":"2022-10-18T16:53:04.152196Z","iopub.status.idle":"2022-10-18T16:53:04.312401Z","shell.execute_reply.started":"2022-10-18T16:53:04.152144Z","shell.execute_reply":"2022-10-18T16:53:04.311030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    \n    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = pattern.sub('', text)\n    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n    emoji = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    text = emoji.sub(r'', text)\n    text = text.lower()    \n    text = expand_contractions(text)\n    text = strip_html(text)\n    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:04.313852Z","iopub.execute_input":"2022-10-18T16:53:04.314627Z","iopub.status.idle":"2022-10-18T16:53:04.323063Z","shell.execute_reply.started":"2022-10-18T16:53:04.314584Z","shell.execute_reply":"2022-10-18T16:53:04.321690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef CleanTokenize(df):\n    head_lines = list()\n    lines = df[\"headline\"].values.tolist()\n\n    for line in lines:\n        line = clean_text(line)\n        # tokenize the text\n        tokens = word_tokenize(line)\n        # remove puntuations\n        table = str.maketrans('', '', string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove non alphabetic characters\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words(\"english\"))\n        # remove stop words\n        words = [w for w in words if not w in stop_words]\n        head_lines.append(words)\n    return head_lines\n\nhead_lines = CleanTokenize(df)\nhead_lines[0:20]","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:04.324813Z","iopub.execute_input":"2022-10-18T16:53:04.325185Z","iopub.status.idle":"2022-10-18T16:53:33.864606Z","shell.execute_reply.started":"2022-10-18T16:53:04.325152Z","shell.execute_reply":"2022-10-18T16:53:33.863489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Most Frequent sarcastic words\nfrom collections import Counter\nfrom wordcloud import WordCloud, ImageColorGenerator\n\npos_data = df.loc[df['is_sarcastic'] == 1]\npos_head_lines = CleanTokenize(pos_data)\npos_lines = [j for sub in pos_head_lines for j in sub] \nword_could_dict=Counter(pos_lines)\n\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:33.866696Z","iopub.execute_input":"2022-10-18T16:53:33.867114Z","iopub.status.idle":"2022-10-18T16:53:48.419526Z","shell.execute_reply.started":"2022-10-18T16:53:33.867077Z","shell.execute_reply":"2022-10-18T16:53:48.418383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[\"is_sarcastic\"] = df[\"is_sarcastic\"].map({0: \"Not Sarcastic\", 1: \"Sarcastic\"})\n# print(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:48.421382Z","iopub.execute_input":"2022-10-18T16:53:48.421786Z","iopub.status.idle":"2022-10-18T16:53:48.426461Z","shell.execute_reply.started":"2022-10-18T16:53:48.421736Z","shell.execute_reply":"2022-10-18T16:53:48.425293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df[[\"headline\", \"is_sarcastic\"]]\n# x = np.array(df[\"headline\"])\n# y = np.array(df[\"is_sarcastic\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:48.428498Z","iopub.execute_input":"2022-10-18T16:53:48.428920Z","iopub.status.idle":"2022-10-18T16:53:48.439632Z","shell.execute_reply.started":"2022-10-18T16:53:48.428885Z","shell.execute_reply":"2022-10-18T16:53:48.438528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_split = 0.2\nmax_length = 25\n\n\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(head_lines)\nsequences = tokenizer_obj.texts_to_sequences(head_lines)\n\nword_index = tokenizer_obj.word_index\nprint(\"unique tokens - \",len(word_index))\nvocab_size = len(tokenizer_obj.word_index) + 1\nprint('vocab size -', vocab_size)\n\nlines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\nsentiment =  df['is_sarcastic'].values\n\nindices = np.arange(lines_pad.shape[0])\nnp.random.shuffle(indices)\nlines_pad = lines_pad[indices]\nsentiment = sentiment[indices]\n\nnum_validation_samples = int(validation_split * lines_pad.shape[0])\n\nX_train_pad = lines_pad[:-num_validation_samples]\ny_train = sentiment[:-num_validation_samples]\nX_test_pad = lines_pad[-num_validation_samples:]\ny_test = sentiment[-num_validation_samples:]","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:48.443646Z","iopub.execute_input":"2022-10-18T16:53:48.444373Z","iopub.status.idle":"2022-10-18T16:53:49.437668Z","shell.execute_reply.started":"2022-10-18T16:53:48.444335Z","shell.execute_reply":"2022-10-18T16:53:49.436426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of X_train_pad:', X_train_pad.shape)\nprint('Shape of y_train:', y_train.shape)\n\nprint('Shape of X_test_pad:', X_test_pad.shape)\nprint('Shape of y_test:', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:49.439046Z","iopub.execute_input":"2022-10-18T16:53:49.439410Z","iopub.status.idle":"2022-10-18T16:53:49.446508Z","shell.execute_reply.started":"2022-10-18T16:53:49.439377Z","shell.execute_reply":"2022-10-18T16:53:49.445089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:49.448050Z","iopub.execute_input":"2022-10-18T16:53:49.449229Z","iopub.status.idle":"2022-10-18T16:53:49.460809Z","shell.execute_reply.started":"2022-10-18T16:53:49.449187Z","shell.execute_reply":"2022-10-18T16:53:49.459353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lb = LabelEncoder()\n# y = lb.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:49.462220Z","iopub.execute_input":"2022-10-18T16:53:49.463084Z","iopub.status.idle":"2022-10-18T16:53:49.473482Z","shell.execute_reply.started":"2022-10-18T16:53:49.463042Z","shell.execute_reply":"2022-10-18T16:53:49.472202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GloVe model**","metadata":{}},{"cell_type":"code","source":"f = pd.read_csv('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt', sep=\" \")\nf.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:53:49.475495Z","iopub.execute_input":"2022-10-18T16:53:49.475955Z","iopub.status.idle":"2022-10-18T16:54:27.330549Z","shell.execute_reply.started":"2022-10-18T16:53:49.475908Z","shell.execute_reply":"2022-10-18T16:54:27.329539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nembedding_dim = 100\n\nf = open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt', \"r\")\nfor line in f:\n    values = line.split()\n    # Each line in the file is a word + 100 integers denoting its vector\n    word = values[0]\n    #The first element of every line is a word & the rest 50 are its array of integers\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:27.332109Z","iopub.execute_input":"2022-10-18T16:54:27.332494Z","iopub.status.idle":"2022-10-18T16:54:56.461533Z","shell.execute_reply.started":"2022-10-18T16:54:27.332460Z","shell.execute_reply":"2022-10-18T16:54:56.460210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index[\"neural\"][:10]","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:56.462788Z","iopub.execute_input":"2022-10-18T16:54:56.463271Z","iopub.status.idle":"2022-10-18T16:54:56.470905Z","shell.execute_reply.started":"2022-10-18T16:54:56.463240Z","shell.execute_reply":"2022-10-18T16:54:56.469867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # **Embedding layer**","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nc = 0\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        c+=1\n        embedding_matrix[i] = embedding_vector\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:56.472121Z","iopub.execute_input":"2022-10-18T16:54:56.473058Z","iopub.status.idle":"2022-10-18T16:54:56.538544Z","shell.execute_reply.started":"2022-10-18T16:54:56.473022Z","shell.execute_reply":"2022-10-18T16:54:56.537419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,\n                            embedding_dim,\n                            weights=[embedding_matrix],\n                            input_length=max_length,\n                            trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:56.540194Z","iopub.execute_input":"2022-10-18T16:54:56.540532Z","iopub.status.idle":"2022-10-18T16:54:56.573809Z","shell.execute_reply.started":"2022-10-18T16:54:56.540500Z","shell.execute_reply":"2022-10-18T16:54:56.572649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Build the RNN**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.25))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\nprint('Summary of the built model...')\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:56.575135Z","iopub.execute_input":"2022-10-18T16:54:56.575464Z","iopub.status.idle":"2022-10-18T16:54:56.897595Z","shell.execute_reply.started":"2022-10-18T16:54:56.575435Z","shell.execute_reply":"2022-10-18T16:54:56.896325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_pad, y_train, batch_size=32, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T16:54:56.899599Z","iopub.execute_input":"2022-10-18T16:54:56.899984Z","iopub.status.idle":"2022-10-18T17:40:29.666003Z","shell.execute_reply.started":"2022-10-18T16:54:56.899940Z","shell.execute_reply":"2022-10-18T17:40:29.664678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot results\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, 'g', label='Training accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'g', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:40:29.667919Z","iopub.execute_input":"2022-10-18T17:40:29.668248Z","iopub.status.idle":"2022-10-18T17:40:29.991532Z","shell.execute_reply.started":"2022-10-18T17:40:29.668219Z","shell.execute_reply":"2022-10-18T17:40:29.989413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_text(s):\n    x_final = pd.DataFrame({\"headline\":[s]})\n    test_lines = CleanTokenize(x_final)\n    test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n    test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n    pred = model.predict(test_review_pad)\n    pred*=100\n    if pred[0][0]>=50: return \"It's a sarcasm!\" \n    else: return \"It's not a sarcasm.\"","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:42:02.260011Z","iopub.execute_input":"2022-10-18T17:42:02.260417Z","iopub.status.idle":"2022-10-18T17:42:02.267281Z","shell.execute_reply.started":"2022-10-18T17:42:02.260387Z","shell.execute_reply":"2022-10-18T17:42:02.266315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_sarcasm(\"I have won 100 million dollar\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:40:30.417444Z","iopub.execute_input":"2022-10-18T17:40:30.418071Z","iopub.status.idle":"2022-10-18T17:40:30.493332Z","shell.execute_reply.started":"2022-10-18T17:40:30.418038Z","shell.execute_reply":"2022-10-18T17:40:30.492207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_sarcasm(\"You are beautiful\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:45:28.151659Z","iopub.execute_input":"2022-10-18T17:45:28.152082Z","iopub.status.idle":"2022-10-18T17:45:28.226462Z","shell.execute_reply.started":"2022-10-18T17:45:28.152050Z","shell.execute_reply":"2022-10-18T17:45:28.225158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_sarcasm(\"You should be dead by now\")","metadata":{"execution":{"iopub.status.busy":"2022-10-18T17:46:57.490667Z","iopub.execute_input":"2022-10-18T17:46:57.491103Z","iopub.status.idle":"2022-10-18T17:46:57.560945Z","shell.execute_reply.started":"2022-10-18T17:46:57.491067Z","shell.execute_reply":"2022-10-18T17:46:57.559780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_sarcasm(\"He is a nice person.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}